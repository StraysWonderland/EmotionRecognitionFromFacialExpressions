{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten  \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses import categorical_crossentropy  \n",
    "from tensorflow.keras.optimizers import Adam  \n",
    "from tensorflow.keras.regularizers import l2 \n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing gpu support\n",
    "#tf.config.list_physical_devices()\n",
    "tf.test.is_gpu_available()\n",
    "#tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       emotion                                             pixels        Usage\n",
      "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
      "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
      "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
      "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
      "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
      "...        ...                                                ...          ...\n",
      "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
      "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
      "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
      "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
      "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
      "\n",
      "[35887 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "emotion_data = pd.read_csv('data/fer2013.csv')\n",
    "print(emotion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,train_y,X_test,test_y=[],[],[],[]  \n",
    "for index, row in emotion_data.iterrows():  \n",
    "    val=row['pixels'].split(\" \")  \n",
    "    if 'Training' in row['Usage']:\n",
    "      X_train.append(np.array(val,'float32'))  \n",
    "      train_y.append(row['emotion'])  \n",
    "    elif 'PublicTest' in row['Usage']:  \n",
    "      X_test.append(np.array(val,'float32'))  \n",
    "      test_y.append(row['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64  \n",
    "num_labels = 7  \n",
    "batch_size = 64  \n",
    "epochs = 124\n",
    "width, height = 48, 48 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train = np.array(X_train,'float32')  \n",
    "train_y = np.array(train_y,'float32')  \n",
    "X_test = np.array(X_test,'float32')  \n",
    "test_y = np.array(test_y,'float32')  \n",
    "train_y= utils.to_categorical(train_y, num_classes=num_labels)  \n",
    "test_y= utils.to_categorical(test_y, num_classes=num_labels)\n",
    "X_train -= np.mean(X_train, axis=0)  \n",
    "X_train /= np.std(X_train, axis=0)  \n",
    "X_test -= np.mean(X_test, axis=0)  \n",
    "X_test /= np.std(X_test, axis=0)  \n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_samples, num_classes = emotion_data.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='same', name='image_array', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Conv2D(filters=96, kernel_size=(5, 5), padding='same'))\n",
    "model.add(Conv2D(filters=96, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='same'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='relu'))  \n",
    "model.add(BatchNormalization())  \n",
    "model.add(GlobalAveragePooling2D())  \n",
    "model.add(Flatten())  \n",
    "model.add(Dense(num_labels, activation='softmax'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_array (Conv2D)         (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 96)        153696    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 96)        230496    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 128)       307328    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 256)         819456    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 3, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 3, 3, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 6,945,799\n",
      "Trainable params: 6,944,199\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=categorical_crossentropy,  \n",
    "              optimizer=Adam(),  \n",
    "              metrics=['accuracy'])  \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    min_delta=0.0005,\n",
    "    patience=11,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "cb_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    min_lr=1e-7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/124\n",
      "28709/28709 [==============================] - 12s 419us/sample - loss: 1.6287 - accuracy: 0.3546 - val_loss: 1.9426 - val_accuracy: 0.4294\n",
      "Epoch 2/124\n",
      "28709/28709 [==============================] - 10s 351us/sample - loss: 1.3869 - accuracy: 0.4625 - val_loss: 1.4441 - val_accuracy: 0.4678\n",
      "Epoch 3/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 1.3009 - accuracy: 0.5010 - val_loss: 1.3178 - val_accuracy: 0.4965\n",
      "Epoch 4/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 1.2493 - accuracy: 0.5249 - val_loss: 1.3033 - val_accuracy: 0.4870\n",
      "Epoch 5/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 1.2093 - accuracy: 0.5384 - val_loss: 1.2224 - val_accuracy: 0.5414\n",
      "Epoch 6/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 1.1764 - accuracy: 0.5578 - val_loss: 1.2874 - val_accuracy: 0.5238\n",
      "Epoch 7/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 1.1444 - accuracy: 0.5693 - val_loss: 1.2087 - val_accuracy: 0.5584\n",
      "Epoch 8/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 1.1212 - accuracy: 0.5746 - val_loss: 1.1816 - val_accuracy: 0.5506\n",
      "Epoch 9/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 1.0975 - accuracy: 0.5871 - val_loss: 1.1606 - val_accuracy: 0.5734\n",
      "Epoch 10/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 1.0756 - accuracy: 0.5968 - val_loss: 1.1335 - val_accuracy: 0.5740\n",
      "Epoch 11/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 1.0564 - accuracy: 0.6006 - val_loss: 1.1127 - val_accuracy: 0.5851\n",
      "Epoch 12/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 1.0390 - accuracy: 0.6079 - val_loss: 1.1261 - val_accuracy: 0.5804\n",
      "Epoch 13/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 1.0227 - accuracy: 0.6183 - val_loss: 1.0990 - val_accuracy: 0.5821\n",
      "Epoch 14/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 1.0028 - accuracy: 0.6242 - val_loss: 1.0554 - val_accuracy: 0.6063\n",
      "Epoch 15/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.9869 - accuracy: 0.6279 - val_loss: 1.1438 - val_accuracy: 0.5823\n",
      "Epoch 16/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.9708 - accuracy: 0.6350 - val_loss: 1.0528 - val_accuracy: 0.6082\n",
      "Epoch 17/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.9565 - accuracy: 0.6422 - val_loss: 1.0574 - val_accuracy: 0.5960\n",
      "Epoch 18/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.9406 - accuracy: 0.6481 - val_loss: 1.0531 - val_accuracy: 0.6099\n",
      "Epoch 19/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.9242 - accuracy: 0.6567 - val_loss: 1.0374 - val_accuracy: 0.6186\n",
      "Epoch 20/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 0.9172 - accuracy: 0.6541 - val_loss: 1.0642 - val_accuracy: 0.6055\n",
      "Epoch 21/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.8973 - accuracy: 0.6639 - val_loss: 1.0436 - val_accuracy: 0.6085\n",
      "Epoch 22/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 0.8894 - accuracy: 0.6661 - val_loss: 1.0388 - val_accuracy: 0.6205\n",
      "Epoch 23/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 0.8759 - accuracy: 0.6718 - val_loss: 1.0090 - val_accuracy: 0.6264\n",
      "Epoch 24/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.8573 - accuracy: 0.6833 - val_loss: 1.0201 - val_accuracy: 0.6264\n",
      "Epoch 25/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.8477 - accuracy: 0.6846 - val_loss: 1.0597 - val_accuracy: 0.6149\n",
      "Epoch 26/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.8384 - accuracy: 0.6856 - val_loss: 1.0692 - val_accuracy: 0.6177\n",
      "Epoch 27/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.8252 - accuracy: 0.6918 - val_loss: 1.0675 - val_accuracy: 0.6108\n",
      "Epoch 28/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 0.8172 - accuracy: 0.6947 - val_loss: 1.0235 - val_accuracy: 0.6319\n",
      "Epoch 29/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 0.7950 - accuracy: 0.7018 - val_loss: 1.0445 - val_accuracy: 0.6199\n",
      "Epoch 30/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.7869 - accuracy: 0.7059 - val_loss: 1.0301 - val_accuracy: 0.6294\n",
      "Epoch 31/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.7778 - accuracy: 0.7116 - val_loss: 1.0145 - val_accuracy: 0.6375\n",
      "Epoch 32/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.7684 - accuracy: 0.7124 - val_loss: 1.0500 - val_accuracy: 0.6339\n",
      "Epoch 33/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 0.7557 - accuracy: 0.7197 - val_loss: 1.0099 - val_accuracy: 0.6395\n",
      "Epoch 34/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.7466 - accuracy: 0.7247 - val_loss: 1.0759 - val_accuracy: 0.6303\n",
      "Epoch 35/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.7389 - accuracy: 0.7261 - val_loss: 1.0623 - val_accuracy: 0.6319\n",
      "Epoch 36/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.7297 - accuracy: 0.7266 - val_loss: 1.0481 - val_accuracy: 0.6317\n",
      "Epoch 37/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.7171 - accuracy: 0.7358 - val_loss: 1.0721 - val_accuracy: 0.6174\n",
      "Epoch 38/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.7050 - accuracy: 0.7384 - val_loss: 1.0402 - val_accuracy: 0.6269\n",
      "Epoch 39/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.6933 - accuracy: 0.7418 - val_loss: 1.0985 - val_accuracy: 0.6317\n",
      "Epoch 40/124\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6938 - accuracy: 0.7412\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "28709/28709 [==============================] - 10s 340us/sample - loss: 0.6940 - accuracy: 0.7410 - val_loss: 1.0241 - val_accuracy: 0.6389\n",
      "Epoch 41/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.6354 - accuracy: 0.7638 - val_loss: 1.0586 - val_accuracy: 0.6411\n",
      "Epoch 42/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.6175 - accuracy: 0.7721 - val_loss: 0.9980 - val_accuracy: 0.6517\n",
      "Epoch 43/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.6011 - accuracy: 0.7742 - val_loss: 1.0297 - val_accuracy: 0.6539\n",
      "Epoch 44/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.5967 - accuracy: 0.7807 - val_loss: 1.0278 - val_accuracy: 0.6475\n",
      "Epoch 45/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.5983 - accuracy: 0.7775 - val_loss: 1.0439 - val_accuracy: 0.6562\n",
      "Epoch 46/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 0.5801 - accuracy: 0.7852 - val_loss: 1.0776 - val_accuracy: 0.6539\n",
      "Epoch 47/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 0.5702 - accuracy: 0.7911 - val_loss: 1.0790 - val_accuracy: 0.6498\n",
      "Epoch 48/124\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.5677 - accuracy: 0.7902 - val_loss: 1.0499 - val_accuracy: 0.6481\n",
      "Epoch 49/124\n",
      "28709/28709 [==============================] - 10s 341us/sample - loss: 0.5618 - accuracy: 0.7934 - val_loss: 1.0537 - val_accuracy: 0.6512\n",
      "Epoch 50/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.5523 - accuracy: 0.7975 - val_loss: 1.0722 - val_accuracy: 0.6489\n",
      "Epoch 51/124\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.5488 - accuracy: 0.7956 - val_loss: 1.0619 - val_accuracy: 0.6486\n",
      "Epoch 52/124\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7978\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "28709/28709 [==============================] - 10s 342us/sample - loss: 0.5440 - accuracy: 0.7979 - val_loss: 1.0471 - val_accuracy: 0.6545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/124\n",
      "28709/28709 [==============================] - 10s 348us/sample - loss: 0.5126 - accuracy: 0.8113 - val_loss: 1.0662 - val_accuracy: 0.6514\n",
      "Epoch 54/124\n",
      "28709/28709 [==============================] - 10s 346us/sample - loss: 0.5048 - accuracy: 0.8140 - val_loss: 1.0770 - val_accuracy: 0.6542\n",
      "Epoch 55/124\n",
      "28709/28709 [==============================] - 10s 350us/sample - loss: 0.4960 - accuracy: 0.8150 - val_loss: 1.0848 - val_accuracy: 0.6576\n",
      "Epoch 56/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 0.4873 - accuracy: 0.8198 - val_loss: 1.1242 - val_accuracy: 0.6528\n",
      "Epoch 57/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.4892 - accuracy: 0.8195 - val_loss: 1.0794 - val_accuracy: 0.6573\n",
      "Epoch 58/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4848 - accuracy: 0.8195 - val_loss: 1.0951 - val_accuracy: 0.6512\n",
      "Epoch 59/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 0.4818 - accuracy: 0.8213 - val_loss: 1.1168 - val_accuracy: 0.6528\n",
      "Epoch 60/124\n",
      "28709/28709 [==============================] - 10s 348us/sample - loss: 0.4778 - accuracy: 0.8224 - val_loss: 1.1164 - val_accuracy: 0.6528\n",
      "Epoch 61/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4697 - accuracy: 0.8269 - val_loss: 1.1036 - val_accuracy: 0.6565\n",
      "Epoch 62/124\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.4673 - accuracy: 0.8273\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4673 - accuracy: 0.8274 - val_loss: 1.1000 - val_accuracy: 0.6570\n",
      "Epoch 63/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 0.4536 - accuracy: 0.8333 - val_loss: 1.1055 - val_accuracy: 0.6595\n",
      "Epoch 64/124\n",
      "28709/28709 [==============================] - 10s 348us/sample - loss: 0.4459 - accuracy: 0.8356 - val_loss: 1.1388 - val_accuracy: 0.6573\n",
      "Epoch 65/124\n",
      "28709/28709 [==============================] - 10s 350us/sample - loss: 0.4453 - accuracy: 0.8367 - val_loss: 1.1161 - val_accuracy: 0.6617\n",
      "Epoch 66/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4398 - accuracy: 0.8374 - val_loss: 1.1290 - val_accuracy: 0.6595\n",
      "Epoch 67/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4423 - accuracy: 0.8374 - val_loss: 1.1333 - val_accuracy: 0.6567\n",
      "Epoch 68/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 0.4342 - accuracy: 0.8414 - val_loss: 1.1266 - val_accuracy: 0.6565\n",
      "Epoch 69/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 0.4365 - accuracy: 0.8397 - val_loss: 1.1308 - val_accuracy: 0.6587\n",
      "Epoch 70/124\n",
      "28709/28709 [==============================] - 10s 346us/sample - loss: 0.4383 - accuracy: 0.8379 - val_loss: 1.1177 - val_accuracy: 0.6617\n",
      "Epoch 71/124\n",
      "28709/28709 [==============================] - 10s 350us/sample - loss: 0.4288 - accuracy: 0.8411 - val_loss: 1.1380 - val_accuracy: 0.6623\n",
      "Epoch 72/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 0.4274 - accuracy: 0.8437 - val_loss: 1.1509 - val_accuracy: 0.6581\n",
      "Epoch 73/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4228 - accuracy: 0.8445 - val_loss: 1.1119 - val_accuracy: 0.6620\n",
      "Epoch 74/124\n",
      "28709/28709 [==============================] - 10s 352us/sample - loss: 0.4250 - accuracy: 0.8445 - val_loss: 1.1301 - val_accuracy: 0.6634\n",
      "Epoch 75/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4284 - accuracy: 0.8410 - val_loss: 1.1550 - val_accuracy: 0.6567\n",
      "Epoch 76/124\n",
      "28709/28709 [==============================] - 10s 350us/sample - loss: 0.4263 - accuracy: 0.8431 - val_loss: 1.1440 - val_accuracy: 0.6604\n",
      "Epoch 77/124\n",
      "28709/28709 [==============================] - 10s 348us/sample - loss: 0.4247 - accuracy: 0.8461 - val_loss: 1.1443 - val_accuracy: 0.6562\n",
      "Epoch 78/124\n",
      "28709/28709 [==============================] - 10s 349us/sample - loss: 0.4148 - accuracy: 0.8438 - val_loss: 1.1459 - val_accuracy: 0.6584\n",
      "Epoch 79/124\n",
      "28709/28709 [==============================] - 10s 350us/sample - loss: 0.4111 - accuracy: 0.8498 - val_loss: 1.1446 - val_accuracy: 0.6565\n",
      "Epoch 80/124\n",
      "28709/28709 [==============================] - 10s 345us/sample - loss: 0.4157 - accuracy: 0.8472 - val_loss: 1.1438 - val_accuracy: 0.6609\n",
      "Epoch 81/124\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8496\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "28709/28709 [==============================] - 10s 343us/sample - loss: 0.4095 - accuracy: 0.8497 - val_loss: 1.1501 - val_accuracy: 0.6601\n",
      "Epoch 82/124\n",
      "28709/28709 [==============================] - 10s 344us/sample - loss: 0.4053 - accuracy: 0.8499 - val_loss: 1.1513 - val_accuracy: 0.6567\n",
      "Epoch 83/124\n",
      "28709/28709 [==============================] - 10s 347us/sample - loss: 0.4106 - accuracy: 0.8509 - val_loss: 1.1456 - val_accuracy: 0.6617\n",
      "Epoch 84/124\n",
      "28709/28709 [==============================] - 10s 348us/sample - loss: 0.4070 - accuracy: 0.8494 - val_loss: 1.1557 - val_accuracy: 0.6567\n",
      "Epoch 85/124\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.4011 - accuracy: 0.8542Restoring model weights from the end of the best epoch.\n",
      "28709/28709 [==============================] - 10s 348us/sample - loss: 0.4011 - accuracy: 0.8542 - val_loss: 1.1576 - val_accuracy: 0.6578\n",
      "Epoch 00085: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f654cabba90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, train_y, \n",
    "          steps_per_epoch=len(X_train) / batch_size, \n",
    "          batch_size=batch_size,  \n",
    "          epochs=epochs,  \n",
    "          verbose=1,  \n",
    "          validation_data=(X_test, test_y),\n",
    "          callbacks=[cb_early_stop,cb_reduce_lr]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_4layer.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_4layer.h5\")\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "#Reading the model from JSON file\n",
    "with open('model_4layer.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()#load the model architecture \n",
    "model = tf.keras.models.model_from_json(json_savedModel)\n",
    "model.load_weights('model_4layer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "         optimizer='SGD',\n",
    "         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on testset:  0.6634159933129006\n"
     ]
    }
   ],
   "source": [
    "score = model.predict(X_test)\n",
    "\n",
    "new_X = [np.argmax(item) for item in score]\n",
    "y_test2 = [np.argmax(item) for item in test_y]\n",
    "\n",
    "accuracy = [(x==y) for x,y in zip(new_X,y_test2)]\n",
    "print(\"accuracy on testset: \" , np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcet",
   "language": "python",
   "name": "lcet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
